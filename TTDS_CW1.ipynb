{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('/home/faruk/Desktop/Uni/UoE/TTDS/Lab1/englishST.txt', \"r\", encoding=\"utf-8-sig\")\n",
    "STwords = [word.rstrip() for word in f.readlines()]\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse('/home/faruk/Desktop/Uni/UoE/TTDS/Lab2/collections/trec.sample.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create inv index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_invindex(file):\n",
    "    \n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    inv_index = defaultdict(list)\n",
    "    docs = []\n",
    "\n",
    "    for idx,child in enumerate(root.findall('.//DOC')):\n",
    "        headline = child.find('.//HEADLINE').text\n",
    "        docnum = child.find('.//DOCNO').text\n",
    "        text = child.find('.//TEXT').text\n",
    "        combined = headline.strip() + \" \" + text\n",
    "        tks = re.findall(r'\\d+\\.\\d+(?:bn|m)?|\\w+', combined)\n",
    "        docs.append(docnum)\n",
    "        tks = [ps.stem(word) for word in tks if word.lower() not in STwords]\n",
    "        for pos, word in enumerate(tks):\n",
    "            if word in inv_index: \n",
    "                if docnum in inv_index[word][1]:\n",
    "                    inv_index[word][1][docnum].append(pos) # If the the word already ocurred in the docnum, append the position of the new occurrence.\n",
    "                else:\n",
    "                    inv_index[word][1][docnum] = [pos] # Else, leave position list untouched.\n",
    "\n",
    "            else:\n",
    "                inv_index[word].append('') # If word is not in inv_index, create key and set frequency to 1.\n",
    "                inv_index[word].append({}) # Declare posting list for new word.   \n",
    "                inv_index[word][1][docnum] = [pos] # For each word, add document ID : [pos] pair.\n",
    "\n",
    "            inv_index[word][0] = len(inv_index[word][1].keys()) # If the word already exists in the inv index, increase the frequency of word.\n",
    "    \n",
    "    return inv_index, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_index, docs = create_invindex('/home/faruk/Desktop/Uni/UoE/TTDS/Lab2/collections/trec.sample.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean search parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word(inv_index,word):\n",
    "    if word in inv_index.keys():\n",
    "        return inv_index[word]\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "#get_word(inv_index,'incom')[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proximity_search(inv_index,word1,word2,distance=1):\n",
    "    #a = inv_index[word1][1]\n",
    "    a = get_word(inv_index,word1)[1]\n",
    "    #b = inv_index[word2][1]\n",
    "    b = get_word(inv_index,word2)[1]\n",
    "    match = a.keys() & b.keys()\n",
    "    newdict = {k:(a[k],b[k]) for k in match}\n",
    "    documents = set()\n",
    "    for key,val in newdict.items():\n",
    "        for i in range(len(val)-1):\n",
    "            for j in range(len(val[i])):\n",
    "                temp = val[i][j]\n",
    "                for k in range(len(val[i+1])):\n",
    "                    temp2 = val[i+1][k]\n",
    "                    if distance == 1:\n",
    "                        if temp2 - temp == distance:\n",
    "                            documents.add(key)\n",
    "                    else:\n",
    "                        if abs(temp2 - temp) <= distance:\n",
    "                            documents.add(key)\n",
    "                        \n",
    "\n",
    "    documents = sorted(list(map(int,documents)))\n",
    "    return documents    \n",
    "\n",
    "proximity_search(inv_index, 'incom', 'tax', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_parser(query,inv_index):\n",
    "    print(query)\n",
    "    query = query.lower()\n",
    "    \n",
    "    if query.find(\" \") == -1 and query.find('#') == -1:\n",
    "        word = ps.stem(query)\n",
    "        docs = get_word(inv_index,word)[1].keys()\n",
    "        return sorted(map(int,docs))\n",
    "    \n",
    "    if query.find('\"') != -1:\n",
    "        tks_query = re.findall(r'\"(.+?)\"',query)[0]\n",
    "        tks_query = tks_query.split()\n",
    "        tks_query = [ps.stem(word) for word in tks_query]\n",
    "        temp1 = tks_query\n",
    "        result = proximity_search(inv_index,tks_query[0],tks_query[1])\n",
    "        if query.find(' and ') != -1:\n",
    "            if query.find('not') != -1:\n",
    "                if query.count('\"') == 4:\n",
    "                    tks_query = re.findall(r'\"(.+?)\"',query)[1]\n",
    "                    tks_query = tks_query.split()\n",
    "                    tks_query = [ps.stem(word) for word in tks_query]\n",
    "                    notword = re.findall(r'not\\s\"(.+?)\"',query)[0].split()\n",
    "                    notword = [ps.stem(word) for word in notword]\n",
    "                    if notword == temp1:\n",
    "                        temp = proximity_search(inv_index,tks_query[0],tks_query[1])\n",
    "                        return sorted(set(temp).difference(set(result)))\n",
    "                    else:\n",
    "                        temp = proximity_search(inv_index,tks_query[0],tks_query[1])\n",
    "                        return sorted(set(result).difference(set(temp)))\n",
    "                \n",
    "                notword = re.findall(r'not\\s(.+?)(?:$|\\s)',query)[0]\n",
    "                w = ps.stem(re.sub(r'\"(.*)\"|and|not|\\s','',query))\n",
    "                w = get_word(inv_index, w)[1].keys()\n",
    "                if notword.find('\"') != -1:\n",
    "                    return sorted(set(map(int,w)).difference(set(result)))\n",
    "                else:\n",
    "                    return sorted(set(result).difference(set(map(int,w))))\n",
    "            \n",
    "            else:\n",
    "                if query.count('\"') == 4:\n",
    "                    tks_query = re.findall(r'\"(.+?)\"',query)[1]\n",
    "                    tks_query = tks_query.split()\n",
    "                    tks_query = [ps.stem(word) for word in tks_query]\n",
    "                    temp = proximity_search(inv_index,tks_query[0],tks_query[1])\n",
    "                    return sorted(set(map(int,temp)).intersection(set(result)))\n",
    "                    \n",
    "                w = ps.stem(re.sub(r'\"(.*)\"|and|\\s','',query))\n",
    "                w = get_word(inv_index, w)[1].keys()\n",
    "                return sorted(set(map(int,w)).intersection(set(result)))\n",
    "        \n",
    "        elif query.find(' or ') != -1:\n",
    "            if query.count('\"') == 4:\n",
    "                tks_query = re.findall(r'\"(.+?)\"',query)[1]\n",
    "                tks_query = tks_query.split()\n",
    "                tks_query = [ps.stem(word) for word in tks_query]\n",
    "                temp = proximity_search(inv_index,tks_query[0],tks_query[1])\n",
    "                return sorted(set(result).union(temp))\n",
    "            else:\n",
    "                w = ps.stem(re.sub(r'\"(.*)\"|or|\\s','',query))\n",
    "                w = get_word(inv_index, w)[1].keys()\n",
    "                return sorted(set(result).union(set(map(int,w))))\n",
    "        \n",
    "        else:\n",
    "            return result\n",
    "        \n",
    "    if query.find('#') != -1:\n",
    "        distance = int(re.findall(r'\\d+', query)[0])\n",
    "        tks_query = re.findall(r'\\((.+)\\)', query)[0]\n",
    "        tks_query = tks_query.split(',')\n",
    "        tks_query = [ps.stem(word.strip()) for word in tks_query]\n",
    "        return proximity_search(inv_index,tks_query[0],tks_query[1],distance=distance)\n",
    "    \n",
    "    if query.find(' and ') != -1:\n",
    "        if query.find('not') != -1:\n",
    "            notword = re.findall(r'not\\s(.+?)(?:$|\\s)',query)[0]\n",
    "            w = re.sub(notword, '', query)\n",
    "            w = ps.stem(re.sub(r'and|not', '', w).strip())\n",
    "            notword = ps.stem(notword)\n",
    "            notword = set(map(int,get_word(inv_index, notword)[1].keys()))\n",
    "            w = set(map(int,get_word(inv_index, w)[1].keys()))\n",
    "            return sorted(w.difference(notword))\n",
    "\n",
    "        else:\n",
    "            w1 = set(map(int,get_word(inv_index,ps.stem(re.findall(r'^\\w+',query)[0]))[1].keys()))\n",
    "            w2 = set(map(int,get_word(inv_index, ps.stem(re.findall(r'and\\s(.+)',query)[0]))[1].keys()))\n",
    "            return sorted(w1.intersection(w2))\n",
    "        \n",
    "    if query.find(' or ') != -1:\n",
    "        w1 = set(map(int,get_word(inv_index,ps.stem(re.findall(r'^\\w+',query)[0]))[1].keys()))\n",
    "        w2 = set(map(int,get_word(inv_index,ps.stem(re.findall(r'or\\s(.+)',query)[0]))[1].keys()))\n",
    "        return sorted(w1.union(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happiness\n",
      "[58, 136, 137, 196, 264, 290, 341, 372, 3329, 3362, 3443, 3474, 3638, 3730, 3773, 3856, 3864, 3913]\n",
      "\n",
      "\n",
      "Edinburgh AND Scotland\n",
      "[351]\n",
      "\n",
      "\n",
      "income and taxes\n",
      "[16, 24, 39, 65, 92, 141, 163, 172, 282, 314, 326, 361, 3327, 3343, 3387, 3405, 3441, 3449, 3490, 3495, 3519, 3533, 3535, 3562, 3582, 3589, 3590, 3596, 3599, 3606, 3608, 3617, 3699, 3705, 3706, 3707, 3708, 3710, 3734, 3816, 3817, 3818, 3829, 3834, 3835, 3937]\n",
      "\n",
      "\n",
      "\"income taxes\"\n",
      "[65, 92, 282, 361, 3405, 3441, 3449, 3562, 3706, 3708, 3734, 3818]\n",
      "\n",
      "\n",
      "#20(income, taxes)\n",
      "[65, 92, 163, 282, 361, 3387, 3405, 3441, 3449, 3490, 3533, 3562, 3589, 3590, 3599, 3606, 3617, 3699, 3705, 3706, 3708, 3734, 3816, 3817, 3818, 3829, 3834, 3835]\n",
      "\n",
      "\n",
      "\"middle east\" AND peace\n",
      "[219, 223, 288, 305, 3549, 3663, 3762, 3766]\n",
      "\n",
      "\n",
      "\"islam religion\"\n",
      "[]\n",
      "\n",
      "\n",
      "\"Financial times\" AND NOT BBC\n",
      "[38, 55, 133, 140, 146, 306, 347, 3580, 3690, 3692, 3740, 3907]\n",
      "\n",
      "\n",
      "#15(dow,stocks)\n",
      "[14, 154]\n"
     ]
    }
   ],
   "source": [
    "print(query_parser('Happiness',inv_index))\n",
    "print('\\n')\n",
    "print(query_parser('Edinburgh AND Scotland',inv_index))\n",
    "print('\\n')\n",
    "print(query_parser('income and taxes',inv_index))\n",
    "print('\\n')\n",
    "print(query_parser('\"income taxes\"',inv_index))\n",
    "print('\\n')\n",
    "print(query_parser('#20(income, taxes)',inv_index))\n",
    "print('\\n')\n",
    "print(query_parser('\"middle east\" AND peace',inv_index))\n",
    "print('\\n')\n",
    "print(query_parser('\"islam religion\"',inv_index))\n",
    "print('\\n')\n",
    "print(query_parser('\"Financial times\" AND NOT BBC',inv_index))\n",
    "print('\\n')\n",
    "print(query_parser('#15(dow,stocks)',inv_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read query and write results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_queries_boolean(file, inv_index):\n",
    "    with open(file, 'r') as file, open('results.boolean.txt', 'w') as outfile:\n",
    "        for i,line in enumerate(file):\n",
    "            data = line.strip()\n",
    "            data = re.sub(r'\\d+','',data).strip()\n",
    "            results = query_parser(data,inv_index)\n",
    "            for result in results:\n",
    "                outfile.write(str(i+1) + ',' + str(result) + '\\n')\n",
    "                \n",
    "read_queries_boolean('collections/queries.lab2.txt', inv_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranked search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tfidf document index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranked_search(query, inv_index, docs):\n",
    "    print(query)\n",
    "    queries = re.findall(r'\\w+',query)\n",
    "    queries = [ps.stem(word) for word in queries if ps.stem(word.lower()) not in STwords]\n",
    "\n",
    "    query_index = {}\n",
    "    tfidf_index = {}\n",
    "    for doc in docs:\n",
    "        tfidf_index[doc] = {} \n",
    "\n",
    "    for word in queries:\n",
    "        query_index[word] = 1 + math.log(queries.count(word),10)\n",
    "        for doc in docs:\n",
    "            # Search which documents contain the word and calculate tfidf, if word not in document, then tfidf=0.\n",
    "                if doc in inv_index[word][1].keys():\n",
    "                    tfidf_index[doc][word] = (1 + math.log(len(inv_index[word][1][doc]),10)) * (math.log((len(docs)/inv_index[word][0]),10))\n",
    "                else:\n",
    "                    tfidf_index[doc][word] = 0       \n",
    "                    \n",
    "    scores = []\n",
    "    for doc in docs:\n",
    "        score = 0\n",
    "        for word in queries:\n",
    "            if tfidf_index[doc][word] != 0:\n",
    "                score += query_index[word] * (tfidf_index[doc][word])\n",
    "        if score > 0:\n",
    "            scores.append((doc,score))\n",
    "    \n",
    "    return sorted(scores, key=lambda tup: tup[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ranked_search(\"income tax reduction\", inv_index, docs)[:10])\n",
    "print('\\n')\n",
    "print(ranked_search(\"stock market in Japan\", inv_index, docs)[:10])\n",
    "print('\\n')\n",
    "print(ranked_search(\"health industry\", inv_index, docs)[:10])\n",
    "print('\\n')\n",
    "print(ranked_search(\"the Robotics industries\", inv_index, docs)[:10])\n",
    "print('\\n')\n",
    "print(ranked_search(\"the peace process in the middle east\", inv_index, docs)[:10])\n",
    "print('\\n')\n",
    "print(ranked_search(\"information retrieval\", inv_index, docs)[:10])\n",
    "print('\\n')\n",
    "print(ranked_search(\"Dow Jones industrial average stocks\", inv_index, docs)[:10])\n",
    "print('\\n')\n",
    "print(ranked_search(\"will be there a reduction in the income taxes?\", inv_index, docs)[:10])\n",
    "print('\\n')\n",
    "print(ranked_search(\"the gold prices versus the dollar price\", inv_index, docs)[:10])\n",
    "print('\\n')\n",
    "print(ranked_search(\"FT article on the BBC and BSkyB deal\", inv_index, docs)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read query and write results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queries_ranked(file, tfidf_index, vocab, doc_index):\n",
    "    res = []\n",
    "    with open(file, 'r') as file, open('results.ranked.txt', 'w') as outfile:\n",
    "        for i,line in enumerate(file):\n",
    "            data = line.strip()\n",
    "            data = re.sub(r'^\\d+\\s','',data).strip()\n",
    "            results = ranked_search(data,tfidf_index, vocab, doc_index)\n",
    "            for j,result in enumerate(results):\n",
    "                if j < 150:\n",
    "                    outfile.write(str(i+1) + ',' + str(result[0]) + ',' + str(round(result[1],4)) + '\\n')\n",
    "                else:\n",
    "                    continue\n",
    "                        \n",
    "queries_ranked('collections/queries.lab3.txt', tfidf,vocab,doc_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TTDS",
   "language": "python",
   "name": "ttds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
